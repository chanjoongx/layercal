/**
 * LayerCal Code Generator v2.0
 * 
 * Features:
 * - Smart grouping of consecutive identical layers
 * - Context-aware BatchNorm (1d/2d)
 * - Semantic variable naming
 * - Full PyTorch, TensorFlow, JAX support
 * - Clean formatting with proper indentation
 */

// ============================================================
// UTILITY FUNCTIONS
// ============================================================

/**
 * Check if two layers have identical type and params
 */
const areLayersIdentical = (layer1, layer2) => {
  if (layer1.type !== layer2.type) return false;
  return JSON.stringify(layer1.params) === JSON.stringify(layer2.params);
};

/**
 * Group consecutive identical layers
 * Returns: [{ type, params, count, startIndex }]
 */
const groupConsecutiveLayers = (layers) => {
  if (layers.length === 0) return [];
  
  const groups = [];
  let i = 0;
  
  while (i < layers.length) {
    const currentLayer = layers[i];
    let count = 1;
    
    // Count consecutive identical layers
    while (i + count < layers.length && areLayersIdentical(currentLayer, layers[i + count])) {
      count++;
    }
    
    groups.push({
      type: currentLayer.type,
      params: currentLayer.params,
      count: count,
      startIndex: i
    });
    
    i += count;
  }
  
  return groups;
};

/**
 * Detect if we're in CNN context (for BatchNorm selection)
 * Only checks the immediately previous layer
 */
const detectContext = (layers, currentIndex) => {
  if (currentIndex <= 0) return 'nlp';
  
  const prevType = layers[currentIndex - 1].type;
  
  // CNN layers
  if (['conv2d', 'maxpool2d', 'avgpool2d'].includes(prevType)) {
    return 'cnn';
  }
  
  // Everything else is considered 1D
  return 'nlp';
};

/**
 * Name counter for semantic naming
 */
const createNameCounter = () => {
  const counts = {};
  return (type) => {
    counts[type] = (counts[type] || 0) + 1;
    return counts[type];
  };
};


// ============================================================
// PYTORCH CODE GENERATOR
// ============================================================

export const generatePyTorchCode = (layers, modelName = 'GeneratedModel') => {
  if (layers.length === 0) {
    return '# Add layers to generate code';
  }

  const groups = groupConsecutiveLayers(layers);
  const getName = createNameCounter();
  
  // Collect definitions and forward lines
  const definitions = [];
  const forwardLines = [];
  
  groups.forEach((group, groupIndex) => {
    const { type, params, count, startIndex } = group;
    const context = detectContext(layers, startIndex);
    const result = generatePyTorchGroup(type, params, count, getName, context);
    
    if (result) {
      definitions.push(...result.definitions);
      forwardLines.push(...result.forwards);
    }
  });

  // Build final code
  let code = `import torch
import torch.nn as nn


class ${modelName}(nn.Module):
    """
    Generated by LayerCal
    https://layercal.com
    """
    
    def __init__(self):
        super().__init__()
`;

  // Add definitions
  if (definitions.length > 0) {
    code += '\n';
    definitions.forEach(def => {
      code += `        ${def}\n`;
    });
  }

  // Add forward method
  code += `

    def forward(self, x):
`;

  forwardLines.forEach(line => {
    code += `        ${line}\n`;
  });

  code += `
        return x


if __name__ == "__main__":
    model = ${modelName}()
    print(model)
    
    # Count parameters
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\\nTotal parameters: {total:,}")
    print(f"Trainable: {trainable:,}")
`;

  return code;
};


const generatePyTorchGroup = (type, params, count, getName, context) => {
  const definitions = [];
  const forwards = [];
  
  switch (type) {
    case 'embedding': {
      const idx = getName('embedding');
      const varName = idx === 1 ? 'embed' : `embed${idx}`;
      definitions.push(`self.${varName} = nn.Embedding(${params.vocab_size}, ${params.embedding_dim})`);
      forwards.push(`x = self.${varName}(x)`);
      break;
    }
    
    case 'linear': {
      const idx = getName('linear');
      const varName = idx === 1 ? 'fc' : `fc${idx}`;
      
      if (count > 1) {
        // Multiple identical linear layers
        const layerDef = `nn.Linear(${params.input_dim}, ${params.output_dim}, bias=${params.use_bias ? 'True' : 'False'})`;
        definitions.push(`self.${varName} = nn.Sequential(`);
        for (let i = 0; i < count; i++) {
          const comma = i < count - 1 ? ',' : '';
          definitions.push(`    ${layerDef}${comma}`);
        }
        definitions.push(`)  # ${count} identical layers`);
        forwards.push(`x = self.${varName}(x)`);
      } else {
        definitions.push(`self.${varName} = nn.Linear(${params.input_dim}, ${params.output_dim}, bias=${params.use_bias ? 'True' : 'False'})`);
        forwards.push(`x = self.${varName}(x)`);
      }
      break;
    }
    
    case 'conv2d': {
      const idx = getName('conv2d');
      const varName = idx === 1 ? 'conv' : `conv${idx}`;
      const padding = Math.floor(params.kernel_size / 2);
      
      if (count > 1) {
        const layerDef = `nn.Conv2d(${params.in_channels}, ${params.out_channels}, ${params.kernel_size}, padding=${padding}, bias=${params.use_bias ? 'True' : 'False'})`;
        definitions.push(`self.${varName} = nn.Sequential(`);
        for (let i = 0; i < count; i++) {
          const comma = i < count - 1 ? ',' : '';
          definitions.push(`    ${layerDef}${comma}`);
        }
        definitions.push(`)  # ${count} identical layers`);
        forwards.push(`x = self.${varName}(x)`);
      } else {
        definitions.push(`self.${varName} = nn.Conv2d(${params.in_channels}, ${params.out_channels}, ${params.kernel_size}, padding=${padding}, bias=${params.use_bias ? 'True' : 'False'})`);
        forwards.push(`x = self.${varName}(x)`);
      }
      break;
    }
    
    case 'lstm': {
      const idx = getName('lstm');
      const varName = idx === 1 ? 'lstm' : `lstm${idx}`;
      const totalLayers = params.num_layers * count;
      const bidir = params.bidirectional ? 'True' : 'False';
      
      definitions.push(`self.${varName} = nn.LSTM(${params.input_size}, ${params.hidden_size}, num_layers=${totalLayers}, batch_first=True, bidirectional=${bidir})`);
      forwards.push(`x, _ = self.${varName}(x)`);
      break;
    }
    
    case 'gru': {
      const idx = getName('gru');
      const varName = idx === 1 ? 'gru' : `gru${idx}`;
      const totalLayers = params.num_layers * count;
      const bidir = params.bidirectional ? 'True' : 'False';
      
      definitions.push(`self.${varName} = nn.GRU(${params.input_size}, ${params.hidden_size}, num_layers=${totalLayers}, batch_first=True, bidirectional=${bidir})`);
      forwards.push(`x, _ = self.${varName}(x)`);
      break;
    }
    
    case 'transformer': {
      const idx = getName('transformer');
      const varName = idx === 1 ? 'transformer' : `transformer${idx}`;
      
      if (count > 1) {
        // Use TransformerEncoder for multiple layers
        definitions.push(`self.${varName}_layer = nn.TransformerEncoderLayer(`);
        definitions.push(`    d_model=${params.d_model}, nhead=${params.num_heads},`);
        definitions.push(`    dim_feedforward=${params.d_ff}, batch_first=True`);
        definitions.push(`)`);
        definitions.push(`self.${varName} = nn.TransformerEncoder(self.${varName}_layer, num_layers=${count})`);
        forwards.push(`x = self.${varName}(x)`);
      } else {
        definitions.push(`self.${varName} = nn.TransformerEncoderLayer(`);
        definitions.push(`    d_model=${params.d_model}, nhead=${params.num_heads},`);
        definitions.push(`    dim_feedforward=${params.d_ff}, batch_first=True`);
        definitions.push(`)`);
        forwards.push(`x = self.${varName}(x)`);
      }
      break;
    }
    
    case 'attention': {
      const idx = getName('attention');
      const varName = idx === 1 ? 'attn' : `attn${idx}`;
      
      definitions.push(`self.${varName} = nn.MultiheadAttention(${params.d_model}, ${params.num_heads}, batch_first=True)`);
      forwards.push(`x, _ = self.${varName}(x, x, x)  # Self-attention`);
      break;
    }
    
    case 'batchnorm': {
      const idx = getName('batchnorm');
      const varName = idx === 1 ? 'bn' : `bn${idx}`;
      const bnType = context === 'cnn' ? 'BatchNorm2d' : 'BatchNorm1d';
      
      if (count > 1) {
        definitions.push(`self.${varName} = nn.Sequential(`);
        for (let i = 0; i < count; i++) {
          const comma = i < count - 1 ? ',' : '';
          definitions.push(`    nn.${bnType}(${params.num_features})${comma}`);
        }
        definitions.push(`)`);
        forwards.push(`x = self.${varName}(x)`);
      } else {
        definitions.push(`self.${varName} = nn.${bnType}(${params.num_features})`);
        forwards.push(`x = self.${varName}(x)`);
      }
      break;
    }
    
    case 'layernorm': {
      const idx = getName('layernorm');
      const varName = idx === 1 ? 'ln' : `ln${idx}`;
      
      definitions.push(`self.${varName} = nn.LayerNorm(${params.normalized_shape})`);
      forwards.push(`x = self.${varName}(x)`);
      break;
    }
    
    case 'dropout': {
      const idx = getName('dropout');
      const varName = idx === 1 ? 'dropout' : `dropout${idx}`;
      
      definitions.push(`self.${varName} = nn.Dropout(${params.rate})`);
      forwards.push(`x = self.${varName}(x)`);
      break;
    }
    
    case 'maxpool2d': {
      const idx = getName('maxpool2d');
      const varName = idx === 1 ? 'maxpool' : `maxpool${idx}`;
      
      definitions.push(`self.${varName} = nn.MaxPool2d(${params.kernel_size}, stride=${params.kernel_size})`);
      forwards.push(`x = self.${varName}(x)`);
      break;
    }
    
    case 'avgpool2d': {
      const idx = getName('avgpool2d');
      const varName = idx === 1 ? 'avgpool' : `avgpool${idx}`;
      
      definitions.push(`self.${varName} = nn.AvgPool2d(${params.kernel_size}, stride=${params.kernel_size})`);
      forwards.push(`x = self.${varName}(x)`);
      break;
    }
    
    case 'relu': {
      const idx = getName('relu');
      const varName = idx === 1 ? 'relu' : `relu${idx}`;
      
      definitions.push(`self.${varName} = nn.ReLU()`);
      forwards.push(`x = self.${varName}(x)`);
      break;
    }
    
    case 'softmax': {
      const idx = getName('softmax');
      const varName = idx === 1 ? 'softmax' : `softmax${idx}`;
      
      definitions.push(`self.${varName} = nn.Softmax(dim=-1)`);
      forwards.push(`x = self.${varName}(x)`);
      break;
    }
    
    default:
      break;
  }
  
  return { definitions, forwards };
};


// ============================================================
// TENSORFLOW CODE GENERATOR
// ============================================================

export const generateTensorFlowCode = (layers, modelName = 'GeneratedModel') => {
  if (layers.length === 0) {
    return '# Add layers to generate code';
  }

  const groups = groupConsecutiveLayers(layers);
  const hasComplexLayer = layers.some(l => ['attention', 'transformer'].includes(l.type));
  
  if (hasComplexLayer) {
    return generateTensorFlowFunctional(layers, groups, modelName);
  }
  
  return generateTensorFlowSequential(layers, groups, modelName);
};


const generateTensorFlowSequential = (layers, groups, modelName) => {
  const layerLines = [];
  
  groups.forEach(group => {
    const { type, params, count } = group;
    const lines = getTensorFlowLayerLines(type, params, count);
    layerLines.push(...lines);
  });

  let code = `import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


def create_${modelName.toLowerCase()}():
    """
    Generated by LayerCal
    https://layercal.com
    """
    model = keras.Sequential([
`;

  layerLines.forEach((line, i) => {
    const comma = i < layerLines.length - 1 ? ',' : '';
    code += `        ${line}${comma}\n`;
  });

  code += `    ])
    return model


if __name__ == "__main__":
    model = create_${modelName.toLowerCase()}()
    model.summary()
`;

  return code;
};


const generateTensorFlowFunctional = (layers, groups, modelName) => {
  let code = `import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


def create_${modelName.toLowerCase()}(input_shape):
    """
    Generated by LayerCal (Functional API)
    https://layercal.com
    
    Uses Functional API for Attention/Transformer support
    """
    inputs = keras.Input(shape=input_shape)
    x = inputs
`;

  groups.forEach(group => {
    const { type, params, count } = group;
    const lines = getTensorFlowFunctionalLines(type, params, count);
    lines.forEach(line => {
      code += `    ${line}\n`;
    });
  });

  code += `
    model = keras.Model(inputs, x, name="${modelName.toLowerCase()}")
    return model


if __name__ == "__main__":
    # Adjust input_shape for your use case
    model = create_${modelName.toLowerCase()}(input_shape=(128,))
    model.summary()
`;

  return code;
};


const getTensorFlowLayerLines = (type, params, count) => {
  const lines = [];
  
  switch (type) {
    case 'embedding':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.Embedding(${params.vocab_size}, ${params.embedding_dim})`);
      }
      break;
    
    case 'linear':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.Dense(${params.output_dim}, use_bias=${params.use_bias ? 'True' : 'False'})`);
      }
      break;
    
    case 'conv2d':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.Conv2D(${params.out_channels}, ${params.kernel_size}, padding="same", use_bias=${params.use_bias ? 'True' : 'False'})`);
      }
      break;
    
    case 'lstm': {
      const totalLayers = params.num_layers * count;
      for (let i = 0; i < totalLayers; i++) {
        const returnSeq = i < totalLayers - 1;
        const lstmDef = `layers.LSTM(${params.hidden_size}${returnSeq ? ', return_sequences=True' : ''})`;
        if (params.bidirectional) {
          lines.push(`layers.Bidirectional(${lstmDef})`);
        } else {
          lines.push(lstmDef);
        }
      }
      break;
    }
    
    case 'gru': {
      const totalLayers = params.num_layers * count;
      for (let i = 0; i < totalLayers; i++) {
        const returnSeq = i < totalLayers - 1;
        const gruDef = `layers.GRU(${params.hidden_size}${returnSeq ? ', return_sequences=True' : ''})`;
        if (params.bidirectional) {
          lines.push(`layers.Bidirectional(${gruDef})`);
        } else {
          lines.push(gruDef);
        }
      }
      break;
    }
    
    case 'batchnorm':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.BatchNormalization()`);
      }
      break;
    
    case 'layernorm':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.LayerNormalization()`);
      }
      break;
    
    case 'dropout':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.Dropout(${params.rate})`);
      }
      break;
    
    case 'maxpool2d':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.MaxPooling2D(pool_size=(${params.kernel_size}, ${params.kernel_size}))`);
      }
      break;
    
    case 'avgpool2d':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.AveragePooling2D(pool_size=(${params.kernel_size}, ${params.kernel_size}))`);
      }
      break;
    
    case 'relu':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.ReLU()`);
      }
      break;
    
    case 'softmax':
      for (let i = 0; i < count; i++) {
        lines.push(`layers.Softmax()`);
      }
      break;
    
    case 'transformer':
      lines.push(`# Transformer requires Functional API - switch detected`);
      break;
    
    case 'attention':
      lines.push(`# Attention requires Functional API - switch detected`);
      break;
  }
  
  return lines;
};


const getTensorFlowFunctionalLines = (type, params, count) => {
  const lines = [];
  
  switch (type) {
    case 'embedding':
      lines.push(`x = layers.Embedding(${params.vocab_size}, ${params.embedding_dim})(x)`);
      break;
    
    case 'linear':
      for (let i = 0; i < count; i++) {
        lines.push(`x = layers.Dense(${params.output_dim}, use_bias=${params.use_bias ? 'True' : 'False'})(x)`);
      }
      break;
    
    case 'conv2d':
      for (let i = 0; i < count; i++) {
        lines.push(`x = layers.Conv2D(${params.out_channels}, ${params.kernel_size}, padding="same", use_bias=${params.use_bias ? 'True' : 'False'})(x)`);
      }
      break;
    
    case 'lstm': {
      const totalLayers = params.num_layers * count;
      for (let i = 0; i < totalLayers; i++) {
        const returnSeq = i < totalLayers - 1;
        const lstmDef = `layers.LSTM(${params.hidden_size}${returnSeq ? ', return_sequences=True' : ''})`;
        if (params.bidirectional) {
          lines.push(`x = layers.Bidirectional(${lstmDef})(x)`);
        } else {
          lines.push(`x = ${lstmDef}(x)`);
        }
      }
      break;
    }
    
    case 'gru': {
      const totalLayers = params.num_layers * count;
      for (let i = 0; i < totalLayers; i++) {
        const returnSeq = i < totalLayers - 1;
        const gruDef = `layers.GRU(${params.hidden_size}${returnSeq ? ', return_sequences=True' : ''})`;
        if (params.bidirectional) {
          lines.push(`x = layers.Bidirectional(${gruDef})(x)`);
        } else {
          lines.push(`x = ${gruDef}(x)`);
        }
      }
      break;
    }
    
    case 'transformer':
      for (let i = 0; i < count; i++) {
        lines.push(``)
        lines.push(`# Transformer Block ${i + 1}`);
        lines.push(`attn_out = layers.MultiHeadAttention(`);
        lines.push(`    num_heads=${params.num_heads},`);
        lines.push(`    key_dim=${Math.floor(params.d_model / params.num_heads)}`);
        lines.push(`)(x, x)`);
        lines.push(`x = layers.LayerNormalization()(x + attn_out)`);
        lines.push(`ffn_out = layers.Dense(${params.d_ff}, activation="relu")(x)`);
        lines.push(`ffn_out = layers.Dense(${params.d_model})(ffn_out)`);
        lines.push(`x = layers.LayerNormalization()(x + ffn_out)`);
      }
      break;
    
    case 'attention':
      for (let i = 0; i < count; i++) {
        lines.push(`attn_out = layers.MultiHeadAttention(`);
        lines.push(`    num_heads=${params.num_heads},`);
        lines.push(`    key_dim=${Math.floor(params.d_model / params.num_heads)}`);
        lines.push(`)(x, x)`);
        lines.push(`x = x + attn_out  # Residual`);
      }
      break;
    
    case 'batchnorm':
      for (let i = 0; i < count; i++) {
        lines.push(`x = layers.BatchNormalization()(x)`);
      }
      break;
    
    case 'layernorm':
      for (let i = 0; i < count; i++) {
        lines.push(`x = layers.LayerNormalization()(x)`);
      }
      break;
    
    case 'dropout':
      lines.push(`x = layers.Dropout(${params.rate})(x)`);
      break;
    
    case 'maxpool2d':
      lines.push(`x = layers.MaxPooling2D(pool_size=(${params.kernel_size}, ${params.kernel_size}))(x)`);
      break;
    
    case 'avgpool2d':
      lines.push(`x = layers.AveragePooling2D(pool_size=(${params.kernel_size}, ${params.kernel_size}))(x)`);
      break;
    
    case 'relu':
      lines.push(`x = layers.ReLU()(x)`);
      break;
    
    case 'softmax':
      lines.push(`x = layers.Softmax()(x)`);
      break;
  }
  
  return lines;
};


// ============================================================
// JAX/FLAX CODE GENERATOR
// ============================================================

export const generateJAXCode = (layers, modelName = 'GeneratedModel') => {
  if (layers.length === 0) {
    return '# Add layers to generate code';
  }

  const groups = groupConsecutiveLayers(layers);
  
  let code = `import jax
import jax.numpy as jnp
from flax import linen as nn
from typing import Sequence


class ${modelName}(nn.Module):
    """
    Generated by LayerCal
    https://layercal.com
    """
    training: bool = True

    @nn.compact
    def __call__(self, x):
`;

  groups.forEach(group => {
    const { type, params, count } = group;
    const lines = getJAXLines(type, params, count);
    lines.forEach(line => {
      code += `        ${line}\n`;
    });
  });

  code += `
        return x


if __name__ == "__main__":
    import jax.random as random
    
    model = ${modelName}()
    key = random.PRNGKey(0)
    
    # Adjust shape for your use case
    x = jnp.ones((1, 128))
    
    params = model.init(key, x)
    
    # Count parameters
    param_count = sum(p.size for p in jax.tree_util.tree_leaves(params))
    print(f"Total parameters: {param_count:,}")
    
    # Forward pass
    y = model.apply(params, x)
    print(f"Output shape: {y.shape}")
`;

  return code;
};


const getJAXLines = (type, params, count) => {
  const lines = [];
  
  switch (type) {
    case 'embedding':
      lines.push(`x = nn.Embed(num_embeddings=${params.vocab_size}, features=${params.embedding_dim})(x)`);
      break;
    
    case 'linear':
      for (let i = 0; i < count; i++) {
        lines.push(`x = nn.Dense(${params.output_dim}, use_bias=${params.use_bias ? 'True' : 'False'})(x)`);
      }
      break;
    
    case 'conv2d':
      for (let i = 0; i < count; i++) {
        lines.push(`x = nn.Conv(features=${params.out_channels}, kernel_size=(${params.kernel_size}, ${params.kernel_size}), padding="SAME")(x)`);
      }
      break;
    
    case 'lstm': {
      const totalLayers = params.num_layers * count;
      lines.push(`# LSTM: ${totalLayers} layer(s)`);
      lines.push(`lstm = nn.RNN(nn.OptimizedLSTMCell(features=${params.hidden_size}))`);
      lines.push(`x, _ = lstm(x)`);
      if (params.bidirectional) {
        lines.push(`# Note: For bidirectional, wrap with nn.Bidirectional`);
      }
      break;
    }
    
    case 'gru': {
      const totalLayers = params.num_layers * count;
      lines.push(`# GRU: ${totalLayers} layer(s)`);
      lines.push(`gru = nn.RNN(nn.GRUCell(features=${params.hidden_size}))`);
      lines.push(`x, _ = gru(x)`);
      break;
    }
    
    case 'transformer':
      for (let i = 0; i < count; i++) {
        lines.push(``);
        lines.push(`# Transformer Block ${i + 1}`);
        lines.push(`attn_out = nn.SelfAttention(num_heads=${params.num_heads})(x)`);
        lines.push(`x = nn.LayerNorm()(x + attn_out)`);
        lines.push(`ffn_out = nn.Dense(${params.d_ff})(x)`);
        lines.push(`ffn_out = nn.relu(ffn_out)`);
        lines.push(`ffn_out = nn.Dense(${params.d_model})(ffn_out)`);
        lines.push(`x = nn.LayerNorm()(x + ffn_out)`);
      }
      break;
    
    case 'attention':
      for (let i = 0; i < count; i++) {
        lines.push(`attn_out = nn.SelfAttention(num_heads=${params.num_heads})(x)`);
        lines.push(`x = x + attn_out  # Residual`);
      }
      break;
    
    case 'batchnorm':
      for (let i = 0; i < count; i++) {
        lines.push(`x = nn.BatchNorm(use_running_average=not self.training)(x)`);
      }
      break;
    
    case 'layernorm':
      for (let i = 0; i < count; i++) {
        lines.push(`x = nn.LayerNorm()(x)`);
      }
      break;
    
    case 'dropout':
      lines.push(`x = nn.Dropout(rate=${params.rate}, deterministic=not self.training)(x)`);
      break;
    
    case 'maxpool2d':
      lines.push(`x = nn.max_pool(x, window_shape=(${params.kernel_size}, ${params.kernel_size}), strides=(${params.kernel_size}, ${params.kernel_size}))`);
      break;
    
    case 'avgpool2d':
      lines.push(`x = nn.avg_pool(x, window_shape=(${params.kernel_size}, ${params.kernel_size}), strides=(${params.kernel_size}, ${params.kernel_size}))`);
      break;
    
    case 'relu':
      lines.push(`x = nn.relu(x)`);
      break;
    
    case 'softmax':
      lines.push(`x = nn.softmax(x, axis=-1)`);
      break;
    
    default:
      lines.push(`# ${type}: Not yet implemented in JAX`);
      break;
  }
  
  return lines;
};